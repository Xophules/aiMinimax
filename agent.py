#!/usr/bin/env python3

from ...lib.game import Agent, RandomAgent, GreedyAgent
import math, random
import numpy as np




class MinimaxAgent(Agent):
	"""An agent that makes decisions using the Minimax algorithm, using a
	evaluation function to approximately guess how good certain states
	are when looking far into the future.

	:param evaluation_function: The function used to make evaluate a
		GameState. Should have the parameters (state, player_id) where
		`state` is the GameState and `player_id` is the ID of the
		player to calculate the expected payoff for.

	:param alpha_beta_pruning: True if you would like to use
		alpha-beta pruning.

	:param max_depth: The maximum depth to search using the minimax
		algorithm, before using estimates generated by the evaluation
		function.
	"""

	def __init__(self, evaluate_function, alpha_beta_pruning=False, max_depth=5):
		super().__init__()
		self.evaluate = evaluate_function
		#print(alpha_beta_pruning)
		self.alpha_beta_pruning = alpha_beta_pruning
		self.max_depth = max_depth


	def decide(self, state):
		# TODO: Implement this agent!
		#
		# Read the documentation in /src/lib/game/_game.py for
		# information on what the decide function does.
		#
		# Do NOT call the soccer evaluation function that you write
		# directly from this function! Instead, use
		# `self.evaluate`. It will behave identically, but will be
		# able to work for multiple games.
		#
		# Do NOT call any SoccerState-specific functions! Assume that
		# you can only see the functions provided in the GameState
		# class.
		#
		# If you would like to see some example agents, check out
		# `/src/lib/game/_agents.py`.
		if not self.alpha_beta_pruning:
			# return self.maxValue(state, state.current_player, 0)
			return self.minimax(state, state.current_player)
		else:
			return self.minimax_with_ab_pruning(state, state.current_player)

	def maxValue(self, state, player, depth):
		#print("Maxing player:", player)
		if depth > self.max_depth or state.is_terminal:
			a = self.evaluate(state, player, self)
			#print("A:", a)
			return a

		v = math.inf * -1
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			val = self.minValue(newState, player, depth + 1)
			#print("Pre max:", v, val)
			v = max(v, val)
			#print("Post max:", v, val)
		return v

	def minValue(self, state, player, depth):
		#print("Minimizing player:", player)
		if depth > self.max_depth or state.is_terminal:
			return self.evaluate(state, player, self)
		v = math.inf

		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			val = self.maxValue(newState, player, depth)
			#print("Pre min:", v, val)
			v = min(v, val)
			#print("Post min:", v, val)
		return v

	def minimax(self, state, player, depth=1):
		# This is the suggested method you use to do minimax.  Assume
		# `state` is the current state, `player` is the player that
		# the agent is representing (NOT the current player in
		# `state`!)  and `depth` is the current depth of recursion.

		# return super().decide(state)
		bestVal = math.inf * -1
		best = None
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			u = self.maxValue(newState, player, 0)
			if u > bestVal:
				bestVal = u
				best = a

		print("Best action for ", state.player, "is", best, "has a val of", bestVal)
		return best

	def maxValueAB(self, state, player, depth=0, a=float('inf'), b=-float('inf')):
		if depth > self.max_depth or state.is_terminal:
			print("Evalutating player", player, state.current_player)
			return self.evaluate(state, player, self)
		v = -math.inf
		for ac in state.actions:
			newState = state.act(ac)
			if not newState:
				continue
			v = max(v, self.minValueAB(newState, player, depth + 1, a, b))
			if v >= b:
				return v
			a = max(a, v)

		return v

	def minValueAB(self, state, player, depth=0, a=float('inf'), b=-float('inf')):
		if depth > self.max_depth or state.is_terminal:
			print("Evalutating player", player, state.current_player)
			return self.evaluate(state, player, self)

		v = math.inf
		for ac in state.actions:
			newState = state.act(ac)
			if not newState:
				continue
			v = min(v, self.maxValueAB(newState, player, depth, a, b))
			if v <= a:
				return v
			b = min(b, v)

		return v

	def minimax_with_ab_pruning(self, state, player, depth=1,
								alpha=float('inf'), beta=-float('inf')):
		bestVal = -math.inf
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			u = self.maxValueAB(newState, player, 0, alpha, beta)
			if u > bestVal:
				bestVal = u
				best = a

		print("Best action for ", player, "is", best, "has a val of", bestVal)
		return best

	def learn(self, states, player_id):
		if states[len(states) - 1].winner is None:
			print("Draw")
		elif states[len(states) - 1].winner == player_id:
			print("Win")
		else:
			print("Loss")



	def predictMinmax(self, state, player_id):
		pass
